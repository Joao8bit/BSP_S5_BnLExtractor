{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "import fr_core_news_md\n",
    "import de_core_news_md\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import Phrases\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing files and NLP - Choose appropriate language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FR\n",
    "nlp = fr_core_news_md.load(disable=['parser', 'ner'])\n",
    "stop_words = set(get_stop_words('french')) | set(stopwords.words('french'))\n",
    "df = pd.read_csv('AdsFullFR3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DE\n",
    "nlp = de_core_news_md.load(disable=['parser', 'ner'])\n",
    "stop_words = set(get_stop_words('german')) | set(stopwords.words('german'))\n",
    "df = pd.read_csv('AdsFullDE3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "  \"\"\"\n",
    "  Cleans the text by removing unnecessary punctuation symbols\n",
    "  \"\"\"\n",
    "  delete_dict = {sp_char: '' for sp_char in string.punctuation}\n",
    "  delete_dict[' '] =' '\n",
    "  table = str.maketrans(delete_dict)\n",
    "  text1 = text.translate(table)\n",
    "  textArr= text1.split()\n",
    "  text2 = ' '.join([w for w in textArr if ( not w.isdigit() and\n",
    "                                           ( not w.isdigit() and len(w)>3))])\n",
    "  return text2.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Description'] = df['Description'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "  \"\"\"\n",
    "  Removes stopwords from a parameter text\n",
    "  \"\"\"\n",
    "  textArr = text.split(' ')\n",
    "  rem_text = \" \".join([i for i in textArr if i not in stop_words])\n",
    "  return rem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Description'] = df['Description'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = df['Description'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test list: ',text_list[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts,allowed_postags=['NOUN', 'ADJ', 'VERB']):\n",
    "\toutput = []\n",
    "\tfor sent in texts:\n",
    "\t\tdoc = nlp(sent)\n",
    "\t\toutput.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "\treturn output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ads = lemmatization(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phrases(tokenized_ads, min_count=25)\n",
    "for idx in range(len(tokenized_ads)):\n",
    "    for token in bigram[tokenized_ads[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            tokenized_ads[idx].append(token)\n",
    "print('List[2]: ',tokenized_ads[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document Term Frequency conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = gensim.corpora.Dictionary(tokenized_ads)\n",
    "id2word.filter_extremes(no_below=20, no_above=0.5)\n",
    "corpus = [id2word.doc2bow(rev) for rev in tokenized_ads]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an LDA Object from the Gensim Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialising LDA model as lda_model (5 topics as default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LDA(corpus=corpus, id2word=id2word,\n",
    "                num_topics=10, random_state=100,\n",
    "                chunksize=1000, passes=100,iterations=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top topics\n",
    "lda_model.print_topics()\n",
    "print('Top topics:',lda_model.top_topics(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda = CoherenceModel(model=lda_model,\n",
    "                                     texts=tokenized_ads, dictionary=id2word ,\n",
    "                                     coherence='u_mass', processes=0)\n",
    "coherence_lda = coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Calculating coherence...')\n",
    "print('Coherence: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional feature to save graph as html file\n",
    "pyLDAvis.save_html(vis, 'lda_fr.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
